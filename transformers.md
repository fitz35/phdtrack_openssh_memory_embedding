# Transformers informations
## transformers units
The transformer_units is the number of units or neurons in the dense layers inside the transformer encoder. Specifically, in the provided code structure you mentioned in earlier queries, this value is used to determine:

    The dimensionality of the outputs of the multi-head attention mechanism.
    The size (number of neurons) of the feed-forward neural network layers within the transformer encoder.

## Training and compilation

You can train and fit a model keras, but here, I wanted to use the exact same classifiers at the other, making impossible the training (because of the output).
> NOTE : We can add a layer to the neural network to transform it as a classifier model, compil it then train/fit it to acceleratee the fit and train it like a word2vec or a random forest