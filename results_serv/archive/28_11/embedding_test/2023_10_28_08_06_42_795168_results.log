2023_10_28_08_06_42 - results_logger - INFO - Passed program params:
2023_10_28_08_06_42 - results_logger - INFO - param[0]: main.py
2023_10_28_08_06_42 - results_logger - INFO - param[1]: -d
2023_10_28_08_06_42 - results_logger - INFO - param[2]: /root/phdtrack/mem2graph/data/26_filtered_chunk_extraction_-e_only-max-entropy_-s_none
2023_10_28_08_06_42 - results_logger - INFO - param[3]: -p
2023_10_28_08_06_42 - results_logger - INFO - param[4]: deeplearning
2023_10_28_08_06_42 - results_logger - INFO - param[5]: -o
2023_10_28_08_06_42 - results_logger - INFO - param[6]: /root/phdtrack/phdtrack_openssh_memory_embedding/results_serv/output/26_filtered_chunk_extraction_-e_only-max-entropy_-s_none
2023_10_28_08_06_42 - results_logger - INFO - param[7]: -otr
2023_10_28_08_06_42 - results_logger - INFO - param[8]: training
2023_10_28_08_06_42 - results_logger - INFO - param[9]: -ots
2023_10_28_08_06_42 - results_logger - INFO - param[10]: validation
2023_10_28_08_06_42 - results_logger - INFO - ///---!!!! Launching embedding pipeline on dataset /root/phdtrack/mem2graph/data/26_filtered_chunk_extraction_-e_only-max-entropy_-s_none !!!!----///
2023_10_28_08_06_42 - results_logger - INFO - Data origins training : {<DataOriginEnum.Training: 'training'>}
2023_10_28_08_06_42 - results_logger - INFO - Data origins testing : {<DataOriginEnum.Validation: 'validation'>}
2023_10_28_08_06_42 - results_logger - INFO - Pipeline start time : 1698480402.8029373 seconds
2023_10_28_08_06_42 - results_logger - INFO - timer for load_samples_and_labels_from_all_csv_files started
2023_10_28_08_06_42 - results_logger - INFO - Loading samples and labels from 20964 files
2023_10_28_08_08_35 - results_logger - INFO - Number of loaded files: 20964
2023_10_28_08_08_35 - results_logger - INFO - Number of empty files: 0
2023_10_28_08_08_37 - results_logger - INFO - Loading samples and labels from 3735 files
2023_10_28_08_08_57 - results_logger - INFO - Number of loaded files: 3735
2023_10_28_08_08_57 - results_logger - INFO - Number of empty files: 0
2023_10_28_08_08_57 - results_logger - INFO - Time elapsed since the begining of load_samples_and_labels_from_all_csv_files: 134.651246000 s
2023_10_28_08_08_57 - results_logger - INFO - !!!!!!!!!!!!! Transformers instance : transformers_word_character_size16_embedding_dim8_transformer_units2_num_heads2_num_transformer_layers2_dropout_rate01_activationrelu !!!!!!!!!!!!!
2023_10_28_08_08_57 - results_logger - INFO - Transformers instance TransformersHyperParams(index=0, word_character_size=16, embedding_dim=8, transformer_units=2, num_heads=2, num_transformer_layers=2, dropout_rate=0.1, activation='relu') already computed
2023_10_28_08_08_57 - results_logger - INFO - !!!!!!!!!!!!! Transformers instance : transformers_word_character_size16_embedding_dim16_transformer_units2_num_heads2_num_transformer_layers2_dropout_rate01_activationrelu !!!!!!!!!!!!!
2023_10_28_08_08_57 - results_logger - INFO - Transformers instance TransformersHyperParams(index=1, word_character_size=16, embedding_dim=16, transformer_units=2, num_heads=2, num_transformer_layers=2, dropout_rate=0.1, activation='relu') already computed
2023_10_28_08_08_57 - results_logger - INFO - !!!!!!!!!!!!! Transformers instance : transformers_word_character_size8_embedding_dim8_transformer_units2_num_heads2_num_transformer_layers2_dropout_rate01_activationrelu !!!!!!!!!!!!!
2023_10_28_08_08_57 - results_logger - INFO - Transformers instance TransformersHyperParams(index=2, word_character_size=8, embedding_dim=8, transformer_units=2, num_heads=2, num_transformer_layers=2, dropout_rate=0.1, activation='relu') already computed
2023_10_28_08_08_57 - results_logger - INFO - !!!!!!!!!!!!! Transformers instance : transformers_word_character_size8_embedding_dim16_transformer_units2_num_heads2_num_transformer_layers2_dropout_rate01_activationrelu !!!!!!!!!!!!!
2023_10_28_08_08_57 - results_logger - INFO - Transformers instance TransformersHyperParams(index=3, word_character_size=8, embedding_dim=16, transformer_units=2, num_heads=2, num_transformer_layers=2, dropout_rate=0.1, activation='relu') already computed
2023_10_28_08_08_57 - results_logger - INFO - !!!!!!!!!!!!! Transformers instance : transformers_word_character_size16_embedding_dim8_transformer_units4_num_heads4_num_transformer_layers4_dropout_rate03_activationrelu !!!!!!!!!!!!!
2023_10_28_08_08_57 - results_logger - INFO - Transformers instance : TransformersHyperParams(index=4, word_character_size=16, embedding_dim=8, transformer_units=4, num_heads=4, num_transformer_layers=4, dropout_rate=0.3, activation='relu')
2023_10_28_08_09_16 - results_logger - INFO - token number for instance 4 (with padding) : 2049
2023_10_28_08_09_17 - results_logger - INFO - encoder summary : Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, None, 1)]         0         
                                                                 
 dense (Dense)               (None, None, 8)           16        
                                                                 
 model (Functional)          (None, 2049, 8)           676       
                                                                 
 model_1 (Functional)        (None, 2049, 8)           676       
                                                                 
 model_2 (Functional)        (None, 2049, 8)           676       
                                                                 
 model_3 (Functional)        (None, 2049, 8)           676       
                                                                 
 global_average_pooling1d (  (None, 8)                 0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense_9 (Dense)             (None, 8)                 72        
                                                                 
=================================================================
Total params: 2792 (10.91 KB)
Trainable params: 2792 (10.91 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

2023_10_28_08_09_17 - results_logger - INFO - timer for transformers training :  started
2023_10_28_10_09_17 - results_logger - ERROR - Timeout error in transformers pipeline 4, skipping
2023_10_28_10_09_17 - results_logger - INFO - !!!!!!!!!!!!! Transformers instance : transformers_word_character_size16_embedding_dim16_transformer_units4_num_heads4_num_transformer_layers4_dropout_rate03_activationrelu !!!!!!!!!!!!!
2023_10_28_10_09_17 - results_logger - INFO - Transformers instance : TransformersHyperParams(index=5, word_character_size=16, embedding_dim=16, transformer_units=4, num_heads=4, num_transformer_layers=4, dropout_rate=0.3, activation='relu')
2023_10_28_10_09_37 - results_logger - INFO - token number for instance 5 (with padding) : 2049
2023_10_28_10_09_37 - results_logger - INFO - encoder summary : Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, None, 1)]         0         
                                                                 
 dense_10 (Dense)            (None, None, 16)          32        
                                                                 
 model_5 (Functional)        (None, 2049, 16)          1300      
                                                                 
 model_6 (Functional)        (None, 2049, 16)          1300      
                                                                 
 model_7 (Functional)        (None, 2049, 16)          1300      
                                                                 
 model_8 (Functional)        (None, 2049, 16)          1300      
                                                                 
 global_average_pooling1d_1  (None, 16)                0         
  (GlobalAveragePooling1D)                                       
                                                                 
 dense_19 (Dense)            (None, 16)                272       
                                                                 
=================================================================
Total params: 5504 (21.50 KB)
Trainable params: 5504 (21.50 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

2023_10_28_10_09_37 - results_logger - INFO - timer for transformers training :  started
2023_10_28_12_09_38 - results_logger - ERROR - Timeout error in transformers pipeline 5, skipping
2023_10_28_12_09_38 - results_logger - INFO - !!!!!!!!!!!!! Transformers instance : transformers_word_character_size8_embedding_dim8_transformer_units4_num_heads4_num_transformer_layers4_dropout_rate03_activationrelu !!!!!!!!!!!!!
2023_10_28_12_09_38 - results_logger - INFO - Transformers instance : TransformersHyperParams(index=6, word_character_size=8, embedding_dim=8, transformer_units=4, num_heads=4, num_transformer_layers=4, dropout_rate=0.3, activation='relu')
2023_10_28_12_10_14 - results_logger - INFO - token number for instance 6 (with padding) : 4098
2023_10_28_12_10_15 - results_logger - INFO - encoder summary : Model: "model_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, None, 1)]         0         
                                                                 
 dense_20 (Dense)            (None, None, 8)           16        
                                                                 
 model_10 (Functional)       (None, 4098, 8)           676       
                                                                 
 model_11 (Functional)       (None, 4098, 8)           676       
                                                                 
 model_12 (Functional)       (None, 4098, 8)           676       
                                                                 
 model_13 (Functional)       (None, 4098, 8)           676       
                                                                 
 global_average_pooling1d_2  (None, 8)                 0         
  (GlobalAveragePooling1D)                                       
                                                                 
 dense_29 (Dense)            (None, 8)                 72        
                                                                 
=================================================================
Total params: 2792 (10.91 KB)
Trainable params: 2792 (10.91 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

2023_10_28_12_10_15 - results_logger - INFO - timer for transformers training :  started
2023_10_28_14_10_15 - results_logger - ERROR - Timeout error in transformers pipeline 6, skipping
2023_10_28_14_10_15 - results_logger - INFO - !!!!!!!!!!!!! Transformers instance : transformers_word_character_size8_embedding_dim16_transformer_units4_num_heads4_num_transformer_layers4_dropout_rate03_activationrelu !!!!!!!!!!!!!
2023_10_28_14_10_15 - results_logger - INFO - Transformers instance : TransformersHyperParams(index=7, word_character_size=8, embedding_dim=16, transformer_units=4, num_heads=4, num_transformer_layers=4, dropout_rate=0.3, activation='relu')
2023_10_28_14_10_50 - results_logger - INFO - token number for instance 7 (with padding) : 4098
2023_10_28_14_10_50 - results_logger - INFO - encoder summary : Model: "model_19"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       [(None, None, 1)]         0         
                                                                 
 dense_30 (Dense)            (None, None, 16)          32        
                                                                 
 model_15 (Functional)       (None, 4098, 16)          1300      
                                                                 
 model_16 (Functional)       (None, 4098, 16)          1300      
                                                                 
 model_17 (Functional)       (None, 4098, 16)          1300      
                                                                 
 model_18 (Functional)       (None, 4098, 16)          1300      
                                                                 
 global_average_pooling1d_3  (None, 16)                0         
  (GlobalAveragePooling1D)                                       
                                                                 
 dense_39 (Dense)            (None, 16)                272       
                                                                 
=================================================================
Total params: 5504 (21.50 KB)
Trainable params: 5504 (21.50 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

2023_10_28_14_10_50 - results_logger - INFO - timer for transformers training :  started
2023_10_28_16_10_53 - results_logger - ERROR - Timeout error in transformers pipeline 7, skipping
