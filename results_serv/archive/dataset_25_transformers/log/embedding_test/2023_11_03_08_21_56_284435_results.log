2023_11_03_08_21_56 - results_logger - INFO - Passed program params:
2023_11_03_08_21_56 - results_logger - INFO - param[0]: main.py
2023_11_03_08_21_56 - results_logger - INFO - param[1]: -d
2023_11_03_08_21_56 - results_logger - INFO - param[2]: /root/phdtrack/mem2graph/data/25_filtered_chunk_extraction_-e_none_-s_activate/
2023_11_03_08_21_56 - results_logger - INFO - param[3]: -p
2023_11_03_08_21_56 - results_logger - INFO - param[4]: deeplearning
2023_11_03_08_21_56 - results_logger - INFO - param[5]: -o
2023_11_03_08_21_56 - results_logger - INFO - param[6]: /root/phdtrack/phdtrack_openssh_memory_embedding/results_serv/output/
2023_11_03_08_21_56 - results_logger - INFO - param[7]: -otr
2023_11_03_08_21_56 - results_logger - INFO - param[8]: training
2023_11_03_08_21_56 - results_logger - INFO - param[9]: -ots
2023_11_03_08_21_56 - results_logger - INFO - param[10]: validation
2023_11_03_08_21_56 - results_logger - INFO - ///---!!!! Launching embedding pipeline on dataset /root/phdtrack/mem2graph/data/25_filtered_chunk_extraction_-e_none_-s_activate/ !!!!----///
2023_11_03_08_21_56 - results_logger - INFO - Data origins training : {<DataOriginEnum.Training: 'training'>}
2023_11_03_08_21_56 - results_logger - INFO - Data origins testing : {<DataOriginEnum.Validation: 'validation'>}
2023_11_03_08_21_56 - results_logger - INFO - Pipeline start time : 1698999716.2941701 seconds
2023_11_03_08_21_56 - results_logger - INFO - timer for load_samples_and_labels_from_all_csv_files started
2023_11_03_08_21_56 - results_logger - INFO - Loading samples and labels from 3735 files
2023_11_03_08_23_37 - results_logger - INFO - Number of loaded files: 3735
2023_11_03_08_23_37 - results_logger - INFO - Number of empty files: 0
2023_11_03_08_23_37 - results_logger - INFO - Loading samples and labels from 20964 files
2023_11_03_08_32_19 - results_logger - INFO - Number of loaded files: 20964
2023_11_03_08_32_19 - results_logger - INFO - Number of empty files: 0
2023_11_03_08_32_22 - results_logger - INFO - Time elapsed since the begining of load_samples_and_labels_from_all_csv_files: 626.247072000 s
2023_11_03_08_32_25 - results_logger - INFO - !!!!!!!!!!!!! Transformers instance : transformers_word_character_size16_embedding_dim8_transformer_units2_num_heads2_num_transformer_layers2_dropout_rate01_activationrelu !!!!!!!!!!!!!
2023_11_03_08_32_25 - results_logger - INFO - Transformers instance : TransformersHyperParams(index=0, word_character_size=16, embedding_dim=8, transformer_units=2, num_heads=2, num_transformer_layers=2, dropout_rate=0.1, activation='relu')
2023_11_03_08_35_31 - results_logger - INFO - token number for instance 0 (with padding) : 271
2023_11_03_08_35_32 - results_logger - INFO - encoder summary : Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, None, 1)]         0         
                                                                 
 dense (Dense)               (None, None, 8)           16        
                                                                 
 model (Functional)          (None, 271, 8)            222       
                                                                 
 model_1 (Functional)        (None, 271, 8)            222       
                                                                 
 global_average_pooling1d (  (None, 8)                 0         
 GlobalAveragePooling1D)                                         
                                                                 
 dense_5 (Dense)             (None, 8)                 72        
                                                                 
=================================================================
Total params: 532 (2.08 KB)
Trainable params: 532 (2.08 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

2023_11_03_08_35_32 - results_logger - INFO - timer for transformers training :  started
2023_11_03_10_08_52 - results_logger - ERROR - Timeout error in transformers pipeline 0, skipping (and marking)
2023_11_03_10_08_53 - results_logger - INFO - !!!!!!!!!!!!! Transformers instance : transformers_word_character_size16_embedding_dim16_transformer_units2_num_heads2_num_transformer_layers2_dropout_rate01_activationrelu !!!!!!!!!!!!!
2023_11_03_10_08_53 - results_logger - INFO - Transformers instance : TransformersHyperParams(index=1, word_character_size=16, embedding_dim=16, transformer_units=2, num_heads=2, num_transformer_layers=2, dropout_rate=0.1, activation='relu')
2023_11_03_10_15_00 - results_logger - INFO - token number for instance 1 (with padding) : 271
2023_11_03_10_15_01 - results_logger - INFO - encoder summary : Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, None, 1)]         0         
                                                                 
 dense_6 (Dense)             (None, None, 16)          32        
                                                                 
 model_3 (Functional)        (None, 271, 16)           430       
                                                                 
 model_4 (Functional)        (None, 271, 16)           430       
                                                                 
 global_average_pooling1d_1  (None, 16)                0         
  (GlobalAveragePooling1D)                                       
                                                                 
 dense_11 (Dense)            (None, 16)                272       
                                                                 
=================================================================
Total params: 1164 (4.55 KB)
Trainable params: 1164 (4.55 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

2023_11_03_10_15_01 - results_logger - INFO - timer for transformers training :  started
2023_11_03_11_48_21 - results_logger - ERROR - Timeout error in transformers pipeline 1, skipping (and marking)
2023_11_03_11_48_21 - results_logger - INFO - !!!!!!!!!!!!! Transformers instance : transformers_word_character_size8_embedding_dim8_transformer_units2_num_heads2_num_transformer_layers2_dropout_rate01_activationrelu !!!!!!!!!!!!!
2023_11_03_11_48_21 - results_logger - INFO - Transformers instance : TransformersHyperParams(index=2, word_character_size=8, embedding_dim=8, transformer_units=2, num_heads=2, num_transformer_layers=2, dropout_rate=0.1, activation='relu')
